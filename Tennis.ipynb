{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"./Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "print('The state for the second agent looks like:', states[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark model\n",
    "\n",
    "First a benchmark model is implemented. Therefore the multi agent problem is reformulated into a single DDPG problem. There is only one policy which receives both state vectors and outputs the actions of both agents. Also the critic receives both states and the actions of both actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class environment_single_wrapper():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.vector_observations = self.env\n",
    "        self.brain_names = self.env.brain_names\n",
    "        \n",
    "    def reset(self, train_mode):\n",
    "        self.env_info = self.env.reset(train_mode)\n",
    "        return self\n",
    "    \n",
    "    def __getitem__(self, brain_name):\n",
    "        self.env_info = self.env_info[brain_name]\n",
    "        self.vector_observations = self.env_info.vector_observations.reshape(1,-1)\n",
    "        self.rewards = np.array([sum(self.env_info.rewards)]).reshape(1,-1)\n",
    "        self.local_done = np.array([max(self.env_info.local_done)]).reshape(1,-1)\n",
    "        return self\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.env_info = self.env.step(action)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000  # replay buffer size\n",
    "BATCH_SIZE = 16        # minibatch size\n",
    "GAMMA = 0.98            # discount factor\n",
    "TAU = 5e-4              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4       # learning rate of the actor \n",
    "LR_CRITIC = 1e-4       # learning rate of the critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pi_network import PiNetwork\n",
    "from q_network import QNetwork\n",
    "\n",
    "actor = PiNetwork(input_size=state_size*2, output_size=action_size*2, layer_size1=128, layer_size2=64, lr=LR_ACTOR)\n",
    "critic = QNetwork(input_size=state_size*2+action_size*2, layer_size1=128, layer_size2=128, lr=LR_CRITIC)\n",
    "\n",
    "from agent import  DeterministicActorCriticAgent\n",
    "from noise import OUNoise, GaussianNoise\n",
    "from replay_buffer import ReplayBuffer\n",
    "from trainer import Trainer\n",
    "\n",
    "\n",
    "memory = ReplayBuffer(state_size, BUFFER_SIZE , BATCH_SIZE, 1337)\n",
    "#noise = OUNoise(action_size*2, 1, 1337, sigma_decay=0.999, sigma=0.5, sigma_min=0.025)\n",
    "noise = GaussianNoise(action_size*2, 1, 1337, sigma=1.0, sigma_decay=0.999, sigma_min=0.05)\n",
    "ac =  DeterministicActorCriticAgent(gamma=GAMMA, tau=TAU, batch_size=BATCH_SIZE, update_every=1,\n",
    "                                    actor=actor, critic=critic, memory=memory, noise=noise)\n",
    "\n",
    "trainer = Trainer(environment_single_wrapper(env), ac)\n",
    "\n",
    "scores = trainer.train(6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "pd.Series(scores, name='DDPG').plot()\n",
    "pd.Series(scores, name='Rolling mean DDPG (length: 100)').rolling(100).mean().plot()\n",
    "plt.title('Score: Tennis environment')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "actor = trainer.agent.actor_local#.state_dict()\n",
    "\n",
    "#path = './coop_policy_network_tennis'\n",
    "#actor = trainer.agent.actor_local.state_dict()\n",
    "#torch.save(trainer.agent.actor_local.state_dict(), path)\n",
    "\n",
    "#actor = PiNetwork(input_size=state_size*2, output_size=action_size*2, layer_size1=128, layer_size2=64, lr=LR_ACTOR)\n",
    "#actor.load_state_dict(torch.load(path))\n",
    "#actor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = environment_single_wrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = []\n",
    "for i in range(1, 100):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = []                  # initialize the score (for each agent)\n",
    "    while True:\n",
    "        #actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = actor(torch.from_numpy(states).float()).cpu().data.numpy()\n",
    "        #actions = trainer.agent.act(states)\n",
    "        #actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores.append(env_info.rewards)                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    \n",
    "    max_score_of_episode = np.max(np.sum(np.array(scores).reshape(-1, rewards.shape[0]), axis=0))\n",
    "    mean_scores.append(max_score_of_episode)\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, max_score_of_episode))\n",
    "\n",
    "print('Mean score over all episodes {}'.format(np.mean(mean_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
